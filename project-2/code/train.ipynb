{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":["xvO1-5Ohn77O","S6l2jjI6VNGs","P9PXH8EQ49YH","F41mHGQYv9T3","FTkioZCaifW9","zQEP8tKd5A2V","rQX-EaPmxCJH"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"no72KtEP3KDu","colab_type":"text"},"source":["# Project Compositionality: Training\n","\n","This notebook runs all the models and code necessary for the project"]},{"cell_type":"code","metadata":{"id":"Vx5uMPzl3GjR","colab_type":"code","outputId":"ce72387e-d772-4dbd-e4ce-d2d32886e02c","executionInfo":{"status":"ok","timestamp":1590337189187,"user_tz":-120,"elapsed":22108,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# MOUNTING GOOGLE DRIVE\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9bWvsw9L5MIW","colab_type":"text"},"source":["## Setup\n","\n","**IMPORTANT!** Please ensure you run this notebook with **GPU** support, enabled in Google Colab via `\"Runtime > Change runtime type\"`. The command below will show the registered GPU.\n","\n","1. Change the working directory to the top-level, containing subfolders like:\n","  - code\n","  - data\n","  - OpenNMT-py (required)/Fairseq (optional)\n","  - logs\n","  - models\n","  - results\n","2. Clone OpenNMT from Github\n","3. Install OpenNMT via `pip install`"]},{"cell_type":"code","metadata":{"id":"6_HMzvRJeDct","colab_type":"code","outputId":"c1dfd93f-25a3-4ee1-8b99-a101ab125ae9","executionInfo":{"status":"ok","timestamp":1590337206664,"user_tz":-120,"elapsed":3450,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BVNgxHNG3YAr","colab_type":"code","outputId":"369f4c85-cf28-4aad-b658-8a637dc0884e","executionInfo":{"status":"ok","timestamp":1590337823709,"user_tz":-120,"elapsed":11007,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["%%bash\n","# Set working directory\n","set WORK_DIR=\"/gdrive/My Drive/Education/Master Data Science/Master/Semester 1/NLP2/NLP2_2 (shared)\"\n","cd $WORK_DIR\n","ls -lah"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 68K\n","drwx------ 1 root root 4.0K May 24 16:19 .\n","drwxr-xr-x 1 root root 4.0K May 24 16:19 ..\n","-rw------- 1 root root 1.1K May 24 16:19 .bash_history\n","-rw-r--r-- 1 root root 3.1K Apr  9  2018 .bashrc\n","drwxr-xr-x 1 root root 4.0K May 20 16:47 .cache\n","drwxr-xr-x 1 root root 4.0K May 24 16:19 .config\n","drwxr-xr-x 3 root root 4.0K May 20 16:14 .gsutil\n","drwxr-xr-x 1 root root 4.0K May 20 16:45 .ipython\n","drwx------ 2 root root 4.0K May 20 16:45 .jupyter\n","drwxr-xr-x 2 root root 4.0K May 24 16:11 .keras\n","drwx------ 1 root root 4.0K May 20 16:45 .local\n","drwxr-xr-x 3 root root 4.0K May 20 16:45 .node-gyp\n","drwxr-xr-x 4 root root 4.0K May 20 16:45 .npm\n","-rw-r--r-- 1 root root  148 Aug 17  2015 .profile\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9XUsNlOP5Ldi","colab_type":"text"},"source":["### OpenNMT - Cloning & Installation\n","\n","If installation results in `Error`, you must restart the runtime in order to use newly installed versions. After runtime restart re-running the notebook from [Setup](#setup), `pip install` will show all modules installed as necessary."]},{"cell_type":"code","metadata":{"id":"RkY3ABxM3ov7","colab_type":"code","colab":{}},"source":["%%bash\n","if [ ! -d \"OpenNMT-py\" ]; then\n","  # Check if OpenNMT-py doesn't exist, if so, git clone.\n","  git clone https://github.com/OpenNMT/OpenNMT-py\n","fi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1noSeeh5JdJ","colab_type":"code","outputId":"cba59f9a-7824-4a45-8941-45127c1b0810","executionInfo":{"status":"ok","timestamp":1589701144236,"user_tz":-120,"elapsed":4931,"user":{"displayName":"Maarten Peters","photoUrl":"","userId":"11422425687656437047"}},"colab":{"base_uri":"https://localhost:8080/","height":683}},"source":["!pip install OpenNMT-py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: OpenNMT-py in /usr/local/lib/python3.6/dist-packages (1.1.1)\n","Requirement already satisfied: pyonmttok==1.*; platform_system == \"Linux\" in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.18.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.12.0)\n","Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (2.2.1)\n","Requirement already satisfied: configargparse in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.2.3)\n","Requirement already satisfied: tqdm~=4.30.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (4.30.0)\n","Requirement already satisfied: torchtext==0.4.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.4.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.1.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.16.0)\n","Requirement already satisfied: waitress in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.4.3)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.5.0+cu101)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.7.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.2.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.10.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.6.0.post3)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.34.2)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.18.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (46.3.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.28.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.4.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.9.0)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (1.1.0)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (7.1.2)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (2.11.2)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (3.1.1)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.2.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2020.4.5.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py) (1.1.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"46uSFTBF7_TR","colab_type":"text"},"source":["### Fairseq - Cloning and install (optional)\n","\n","As OpenNMT-py does not provide sufficient support for convolutional sequence to sequence models, Fairseq is used to set up a model comparable to [Compositionality Decomposed (Hupkes et al., 2020)](https://arxiv.org/abs/1908.08351) and [Convolutional sequence to sequence learning (Gehring et al., 2017)](https://dl.acm.org/doi/10.5555/3305381.3305510).\n","\n","As Fairseq is not well supported within Google Colab, the installation requires some different installation procedures compared to regular Python packages."]},{"cell_type":"code","metadata":{"id":"r9BgZrKl_Wef","colab_type":"code","outputId":"2315a098-f445-4475-a29a-35bb72b1248d","executionInfo":{"status":"ok","timestamp":1590337875972,"user_tz":-120,"elapsed":432,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%bash\n","if [ ! -d \"Fairseq\" ]; then\n","  # Check if Fairseq doesn't exist, if so, git clone.\n","  git clone https://github.com/pytorch/fairseq\n","fi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fatal: destination path 'fairseq' already exists and is not an empty directory.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CKlJA5r15jvH","colab_type":"code","outputId":"7ab1a331-8696-4f63-b071-85e56c11695b","executionInfo":{"status":"ok","timestamp":1590337961455,"user_tz":-120,"elapsed":76312,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":530}},"source":["# Installing editable version of Fairseq over Github repository\n","!pip install fairseq --editable ./fairseq/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Obtaining file:///gdrive/My%20Drive/Education/Master%20Data%20Science/Master/Semester%201/NLP2/NLP2_2%20%28shared%29/fairseq\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting fairseq\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n","\u001b[K     |████████████████████████████████| 307kB 3.4MB/s \n","\u001b[?25hCollecting sacrebleu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n","\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (1.14.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (1.18.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (4.41.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (0.29.18)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (2019.12.20)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq==0.9.0) (1.5.0+cu101)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq==0.9.0) (3.6.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==0.9.0) (2.20)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq==0.9.0) (0.16.0)\n","Building wheels for collected packages: fairseq\n","  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021129 sha256=2084eac76df5d5f0105b7ffdbd78251cab5f75367484a470c1ef7200ea9aadb8\n","  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n","Successfully built fairseq\n","Installing collected packages: portalocker, sacrebleu, fairseq\n","Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2o7QjnH_g2J1","colab_type":"code","outputId":"b911ca7b-34a4-4e28-8c5d-8bca4172f0bf","executionInfo":{"status":"ok","timestamp":1590337993509,"user_tz":-120,"elapsed":3986,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["%%bash\n","# Build the package from pypi locally to support Nvidia GPUs\n","cd ./fairseq/\n","python setup.py build_ext --inplace\n","cd $WORK_DIR"],"execution_count":0,"outputs":[{"output_type":"stream","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","running build_ext\n","skipping 'fairseq/data/data_utils_fast.cpp' Cython extension (up-to-date)\n","skipping 'fairseq/data/token_block_utils_fast.cpp' Cython extension (up-to-date)\n","copying build/lib.linux-x86_64-3.6/fairseq/libbleu.cpython-36m-x86_64-linux-gnu.so -> fairseq\n","copying build/lib.linux-x86_64-3.6/fairseq/data/data_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\n","copying build/lib.linux-x86_64-3.6/fairseq/data/token_block_utils_fast.cpython-36m-x86_64-linux-gnu.so -> fairseq/data\n","copying build/lib.linux-x86_64-3.6/fairseq/libnat.cpython-36m-x86_64-linux-gnu.so -> fairseq\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"xjG-5vw85aWB","colab_type":"text"},"source":["### Notebook seeds, models and parameters\n","\n","These are the seeds, models and parameters used in all steps of this notebook."]},{"cell_type":"code","metadata":{"id":"rGe9_U6R5erq","colab_type":"code","colab":{}},"source":["seeds = [0,1,2]\n","models = [\"lsmts2s\",\"grus2s\",\"transformer\",\"convs2s\"]\n","data_dir = \"data\"\n","results_dir = \"results\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0EhUuVbsu73","colab_type":"text"},"source":["### Logging\n","\n","Enabling Tensorboard support."]},{"cell_type":"code","metadata":{"id":"WhwUzROLkmRP","colab_type":"code","colab":{}},"source":["!pip install tensorboardX"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJOokINvszLT","colab_type":"code","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTn1DqGbjChy","colab_type":"code","outputId":"39d66d73-df6b-40ae-a2ba-4e93589cbddb","executionInfo":{"status":"ok","timestamp":1590338386738,"user_tz":-120,"elapsed":60534,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorboard --logdir logs"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":["ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 819."]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"MYGNVciv8jxk","colab_type":"text"},"source":["## **Preprocessing**\n","\n","As the training data needs transformations and preprocessing, we will outline the data flow below.\n","\n","1. The PCFG datasets are stored with the following file/directory structure with `data`:\n","  - Final:\n","    - All:\n","      - 'Training files' src.txt & tgt.txt\n","      - 'Development (validation) files' src.txt & tgt.txt\n","      - 'Test files' src.txt & tgt.txt\n","    - Productivity:\n","      - Tasks:\n","        - Test1:\n","          - 'Training files'\n","        - Test2:\n","          - 'Training files'\n","        - 'Test files'\n","      - Whens:\n","        - Test3: 'Training files'\n","        - Test4: 'Training files'\n","        - Test5: 'Training files'\n","        - 'Test files'\n","    - Systematicity:\n","      - Test1:\n","        - 'Training files'\n","        - 'Test files'\n","    - README.txt\n","2. We will copy and rename some of these files, as OpenNMT-py and Fairsec parameters rely on file extensions and common directories. The new files will be placed in the `Transformed` directory.\n","3. Finally, Fairseq and OpenNMT-py will be used to preprocess the data for training with their respective seed values.\n"]},{"cell_type":"markdown","metadata":{"id":"xvO1-5Ohn77O","colab_type":"text"},"source":["### 1. Datasets and Analysis"]},{"cell_type":"code","metadata":{"id":"HjTmufigk3bq","colab_type":"code","outputId":"9ac2dca5-cadd-4c11-9c92-0b8c80de4553","executionInfo":{"status":"ok","timestamp":1590339846023,"user_tz":-120,"elapsed":463,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":918}},"source":["# Printing the directory and file structure of the data\n","import os, sys, yaml\n","\n","original_dir = f\"{data_dir}/Final\"\n","\n","def dir_to_dict(path):\n","    directory = {}\n","    for dirname, dirnames, filenames in os.walk(path):\n","        dn = os.path.basename(dirname)\n","        directory[dn] = []\n","        if dirnames:\n","            for d in dirnames:\n","                directory[dn].append(dir_to_dict(path=os.path.join(path, d)))\n","            for f in filenames:\n","                directory[dn].append(f)\n","        else:\n","            directory[dn] = filenames\n","        return directory\n","\n","print(yaml.dump(dir_to_dict(path=original_dir), default_flow_style=False))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Final:\n","- All:\n","  - train30k_src_all.txt\n","  - train30k_tgt_all.txt\n","  - dev5k_src_all.txt\n","  - dev5k_tgt_all.txt\n","  - test5k_src_all.txt\n","  - test5k_tgt_all.txt\n","- Productivity:\n","  - Tasks:\n","    - Test1:\n","      - prod1_train_src_12tasks.txt\n","      - prod1_train_tgt_12tasks.txt\n","    - Test2:\n","      - prod2_train_src_13tasks.txt\n","      - prod2_train_tgt_13tasks.txt\n","    - prod_test_src_1tasks.txt\n","    - prod_test_tgt_1tasks.txt\n","    - prod_test_src_2tasks.txt\n","    - prod_test_tgt_2tasks.txt\n","    - prod_test_src_3tasks.txt\n","    - prod_test_tgt_3tasks.txt\n","  - Whens:\n","    - Test3:\n","      - prod3_train_src_012whens.txt\n","      - prod3_train_tgt_012whens.txt\n","    - Test4:\n","      - prod4_train_src_0123whens.txt\n","      - prod4_train_tgt_0123whens.txt\n","    - Test5:\n","      - prod5_train_src_024whens.txt\n","      - prod5_train_tgt_024whens.txt\n","    - prod_test_src_0whens.txt\n","    - prod_test_tgt_0whens.txt\n","    - prod_test_src_1whens.txt\n","    - prod_test_tgt_1whens.txt\n","    - prod_test_src_2whens.txt\n","    - prod_test_tgt_2whens.txt\n","    - prod_test_src_3whens.txt\n","    - prod_test_tgt_3whens.txt\n","    - prod_test_src_4whens.txt\n","    - prod_test_tgt_4whens.txt\n","- Systematicity:\n","  - Test1:\n","    - syst1_test_src_exc012.txt\n","    - syst1_test_src_inc012.txt\n","    - syst1_test_tgt_exc012.txt\n","    - syst1_test_tgt_inc012.txt\n","    - syst1_train_src_exc012.txt\n","    - syst1_train_tgt_exc012.txt\n","- .ipynb_checkpoints: []\n","- README.txt\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y0fAQvs0wrky","colab_type":"code","colab":{}},"source":["def get_unique_words(data):\n","  unique_words = set()\n","  for src, _ in data:\n","    unique_words.update(src.split())\n","  return unique_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1yCz2QKwUwd","colab_type":"code","outputId":"5fb62ba7-5a25-4ce8-e1dd-534df22f241c","executionInfo":{"status":"ok","timestamp":1590339881080,"user_tz":-120,"elapsed":463,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# Analyzing training data to get accurate vocabulary sizes\n","all_dir = f\"{original_dir}/All/\"\n","files = [f for f in os.listdir(all_dir) if os.path.isfile(os.path.join(all_dir, f))]\n","\n","for f in [f for f in files if \".txt\" in f]:\n","  file_name = f.split(\"/\")[-1]\n","  with open(os.path.join(all_dir, f)) as input_file:\n","    data = [(line, \"\") for line in input_file.readlines()]\n","    unique_words = get_unique_words(data)\n","    vocab_size = len(unique_words)\n","    print(f\"{file_name} contains {vocab_size} words: \", sorted(unique_words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train30k_src_all.txt contains 66 words:  ['at', 'doing0', 'doing1', 'doing10', 'doing11', 'doing12', 'doing13', 'doing14', 'doing15', 'doing16', 'doing17', 'doing18', 'doing19', 'doing2', 'doing3', 'doing4', 'doing5', 'doing6', 'doing7', 'doing8', 'doing9', 'location0', 'location1', 'location10', 'location11', 'location12', 'location13', 'location14', 'location15', 'location16', 'location17', 'location18', 'location19', 'location2', 'location3', 'location4', 'location5', 'location6', 'location7', 'location8', 'location9', 'person0', 'person1', 'person10', 'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person18', 'person19', 'person2', 'person3', 'person4', 'person5', 'person6', 'person7', 'person8', 'person9', 'went', 'what', 'when', 'where', 'who']\n","train30k_tgt_all.txt contains 62 words:  ['at', 'doing0', 'doing1', 'doing10', 'doing11', 'doing12', 'doing13', 'doing14', 'doing15', 'doing16', 'doing17', 'doing18', 'doing19', 'doing2', 'doing3', 'doing4', 'doing5', 'doing6', 'doing7', 'doing8', 'doing9', 'location0', 'location1', 'location10', 'location11', 'location12', 'location13', 'location14', 'location15', 'location16', 'location17', 'location18', 'location19', 'location2', 'location3', 'location4', 'location5', 'location6', 'location7', 'location8', 'location9', 'person0', 'person1', 'person10', 'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person18', 'person19', 'person2', 'person3', 'person4', 'person5', 'person6', 'person7', 'person8', 'person9', 'went']\n","dev5k_src_all.txt contains 66 words:  ['at', 'doing0', 'doing1', 'doing10', 'doing11', 'doing12', 'doing13', 'doing14', 'doing15', 'doing16', 'doing17', 'doing18', 'doing19', 'doing2', 'doing3', 'doing4', 'doing5', 'doing6', 'doing7', 'doing8', 'doing9', 'location0', 'location1', 'location10', 'location11', 'location12', 'location13', 'location14', 'location15', 'location16', 'location17', 'location18', 'location19', 'location2', 'location3', 'location4', 'location5', 'location6', 'location7', 'location8', 'location9', 'person0', 'person1', 'person10', 'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person18', 'person19', 'person2', 'person3', 'person4', 'person5', 'person6', 'person7', 'person8', 'person9', 'went', 'what', 'when', 'where', 'who']\n","dev5k_tgt_all.txt contains 62 words:  ['at', 'doing0', 'doing1', 'doing10', 'doing11', 'doing12', 'doing13', 'doing14', 'doing15', 'doing16', 'doing17', 'doing18', 'doing19', 'doing2', 'doing3', 'doing4', 'doing5', 'doing6', 'doing7', 'doing8', 'doing9', 'location0', 'location1', 'location10', 'location11', 'location12', 'location13', 'location14', 'location15', 'location16', 'location17', 'location18', 'location19', 'location2', 'location3', 'location4', 'location5', 'location6', 'location7', 'location8', 'location9', 'person0', 'person1', 'person10', 'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person18', 'person19', 'person2', 'person3', 'person4', 'person5', 'person6', 'person7', 'person8', 'person9', 'went']\n","test5k_src_all.txt contains 66 words:  ['at', 'doing0', 'doing1', 'doing10', 'doing11', 'doing12', 'doing13', 'doing14', 'doing15', 'doing16', 'doing17', 'doing18', 'doing19', 'doing2', 'doing3', 'doing4', 'doing5', 'doing6', 'doing7', 'doing8', 'doing9', 'location0', 'location1', 'location10', 'location11', 'location12', 'location13', 'location14', 'location15', 'location16', 'location17', 'location18', 'location19', 'location2', 'location3', 'location4', 'location5', 'location6', 'location7', 'location8', 'location9', 'person0', 'person1', 'person10', 'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person18', 'person19', 'person2', 'person3', 'person4', 'person5', 'person6', 'person7', 'person8', 'person9', 'went', 'what', 'when', 'where', 'who']\n","test5k_tgt_all.txt contains 62 words:  ['at', 'doing0', 'doing1', 'doing10', 'doing11', 'doing12', 'doing13', 'doing14', 'doing15', 'doing16', 'doing17', 'doing18', 'doing19', 'doing2', 'doing3', 'doing4', 'doing5', 'doing6', 'doing7', 'doing8', 'doing9', 'location0', 'location1', 'location10', 'location11', 'location12', 'location13', 'location14', 'location15', 'location16', 'location17', 'location18', 'location19', 'location2', 'location3', 'location4', 'location5', 'location6', 'location7', 'location8', 'location9', 'person0', 'person1', 'person10', 'person11', 'person12', 'person13', 'person14', 'person15', 'person16', 'person17', 'person18', 'person19', 'person2', 'person3', 'person4', 'person5', 'person6', 'person7', 'person8', 'person9', 'went']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S6l2jjI6VNGs","colab_type":"text"},"source":["### 2. Transform files\n","\n","To facilitate training scripts, copy all files and rename datasets to corresponding `.src` and `.tgt` files."]},{"cell_type":"code","metadata":{"id":"htm_CyNCe5me","colab_type":"code","colab":{}},"source":["from distutils.dir_util import copy_tree\n","transformed_dir = f\"{data_dir}/Transformed\"\n","\n","copy_data = False\n","\n","if copy_data:\n","  copy_tree(original_dir, transformed_dir)\n","  for root, dirs, files in os.walk(transformed_dir, topdown=False):\n","    for name in [f for f in files if \".txt\" in f and \"README.txt\" not in f]:\n","        source_file = os.path.join(root, name)\n","        source_file_renamed = source_file\n","        if \"_src_\" in source_file:\n","            source_file_renamed = source_file_renamed.replace(\"_src_\",\"_\").replace(\".txt\", \".src\")\n","        elif \"_tgt_\" in source_file:\n","            source_file_renamed = source_file_renamed.replace(\"_tgt_\",\"_\").replace(\".txt\", \".tgt\")\n","        os.rename(source_file, source_file_renamed)\n","        print(f\"Source '{source_file}' renamed to '{source_file_renamed}'\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ifl-Bg1nVxIg","colab_type":"text"},"source":["### 3.a Create preprocessing shell scripts\n","\n","As preprocessing is done for multiple seeds to account for random sampling, we divide the original datasets to seed specific datasets, called `'runs'` for models."]},{"cell_type":"code","metadata":{"id":"_JXd2C1AqCVw","colab_type":"code","outputId":"5465da34-2976-46bd-ece4-b97d55e8207f","executionInfo":{"status":"ok","timestamp":1590340310053,"user_tz":-120,"elapsed":463,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from pprint import pprint\n","\n","# Create list of datasets to preprocess\n","datasets = []\n","transformed_dir = f\"{data_dir}/Transformed\"\n","dev_set = f\"{transformed_dir}/All/dev5k_all\"\n","\n","for root, dirs, files in os.walk(transformed_dir, topdown=False):\n","  for name in [f for f in files if \".txt\" not in f and \"dev5k\" not in f]:\n","      source_file = os.path.join(root, name)\n","      source_path = source_file.split(\".\")[0]\n","      opennmt_dest_path = source_path.replace(\"/Transformed/\",\"/OpenNMT/\")\n","      fairseq_dest_path = source_path.replace(\"/Transformed/\",\"/Fairseq/\")\n","      datasets.append((source_path, dev_set, opennmt_dest_path, fairseq_dest_path))\n","      \n","datasets = list(set(datasets))\n","\n","pprint(datasets)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cqBklGoGvik8","colab_type":"code","colab":{}},"source":["# Remove existing bash files and write new ones\n","try:\n","  os.remove(\"code/fairseq_preprocess.sh\")\n","except:\n","  pass\n","\n","try:\n","  os.remove(\"code/opennmt_preprocess.sh\")\n","except:\n","  pass\n","\n","for seed in seeds:\n","  for source_path, dev_path, opennmt_dest_path, fairseq_dest_path in datasets:\n","    if \"train\" in source_path:\n","      try:\n","        os.makedirs(opennmt_dest_path)\n","      except:\n","        pass\n","      try:\n","        os.makedirs(fairseq_dest_path)\n","      except:\n","        pass\n","      with open(\"code/fairseq_preprocess.sh\", \"a\") as scriptfile:\n","        scriptfile.write(f'python fairseq/preprocess.py  --source-lang src --target-lang tgt --trainpref \"{source_path}\" --validpref \"{dev_path}\" --destdir \"{fairseq_dest_path}\" --seed {seed}\\n')\n","      with open(\"code/opennmt_preprocess.sh\", \"a\") as scriptfile:\n","        scriptfile.write(f'python OpenNMT-py/preprocess.py  -train_src \"{source_path}.src\" -train_tgt \"{source_path}.tgt\" -valid_src \"{dev_path}.src\" -valid_tgt \"{dev_path}.tgt\" -save_data \"{opennmt_dest_path}\" -src_vocab_size 68 -tgt_vocab_size 66 --seed {seed}\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZBdNfGEHvxE","colab_type":"text"},"source":["### 3.b Preprocess OpenNMT & Fairseq data"]},{"cell_type":"code","metadata":{"id":"9v5ITxA-_8wb","colab_type":"code","colab":{}},"source":["!chmod +x ./code/fairseq_preprocess.sh\n","!./code/fairseq_preprocess.sh"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKyffxAMH56O","colab_type":"code","colab":{}},"source":["!chmod +x ./code/opennmt_preprocess.sh\n","!./code/opennmt_preprocess.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plIY1DVDeOcy","colab_type":"text"},"source":["## Model and Training Script Creation \n","\n","For this project the following *hints* were given as models to train. For our research we focused on a bi-directional LSTM and GRU model, both with two layers and a 512 hidden unit and word embedding space. As these were baseline models, we will also look into a convolutional model, a Transformer and an optimized LSTM but these require significantly more training time.\n","\n","Hupkes et al.(2019) LSTMS2S of Klein et al.(2018) (2 layers)\n","- (+) sequential nature useful for local encodings\n","- (-) long-distance encodings complicated due to sequential nature\n","\n","ConvS2S of Gehring et al.(2017) (15 layers)\n","+ (+) local convolutions useful for local encodings\n","+ (+) layered architecture eases modelling hierarchy\n","- (-) long-distance encodings complicated due to local convolutions\n","\n","Transformer of Vaswani et al.(2017) (6 layers)\n","+ (+) long-distance encodings easy due to attention\n","+ (+) layered architecture eases modelling hierarchy\n","- (-) out-of-order encoding complicate local encoding\n","\n","### OpenNMT: Hyperparameters \n","\n","- The two models to compare;\n","- Hidden dimensionalities (e.g. 128, 256, 512);\n","- Number of stacked layers in enc./dec. (e.g. 2-6);\n","- Regularisation (e.g. dropout of 0.1-0.3);\n","- Balance model size, training speed, and dataset size\n","\n","Experiment with hyperparameters: A full grid search is not necessary"]},{"cell_type":"code","metadata":{"id":"uJV1PgduK9So","colab_type":"code","colab":{}},"source":["# Create model directories\n","for seed in seeds:\n","  for model in models:\n","    for _, _, opennmt_dest_path, _ in datasets:\n","      try:\n","        os.makedirs(opennmt_dest_path.replace(f\"{data_dir}/OpenNMT\",f\"models/{model}/run_{seed}\"))\n","      except:\n","        pass\n","      try:\n","        os.makedirs(opennmt_dest_path.replace(f\"{data_dir}/OpenNMT\",f\"logs/{model}/run_{seed}\"))\n","      except:\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9PXH8EQ49YH","colab_type":"text"},"source":["### LSTMS2S\n","\n","As the first model to train, we will look at a bi-directional Long Short-Term Memory recurrent neural network, or LSTMS2S in here for short. We will create a seperate command for each seed and model task and combine them as a `bash` file, which will train all `runs` and tasks when executed.\n","\n","Training can be monitored at the [Logging](#logging) of this notebook with the Tensorboard extension."]},{"cell_type":"code","metadata":{"id":"6VrzDj7zKQnO","colab_type":"code","colab":{}},"source":["# Create training scripts\n","\n","try:\n","  os.remove(\"code/lstms2s_train.sh\")\n","except:\n","  pass\n","  \n","for seed in seeds:\n","  for source_path, dev_path, dest_path, _ in datasets:\n","    if \"train\" in source_path:\n","      with open(\"code/lstms2s_train.sh\", \"a\") as scriptfile:\n","        data_path = dest_path\n","        model_path = dest_path.replace(f\"{data_dir}/OpenNMT\", f\"models/lstms2s/run_{seed}\")\n","        log_path = dest_path.replace(f\"{data_dir}/OpenNMT\", f\"logs/lstms2s/run_{seed}\")\n","        scriptfile.write(f\"\"\"\n","python OpenNMT-py/train.py  -data \"{data_path}\" -save_model \"{model_path}\" \\\\\n","                            -train_steps 25000 -layers 2 -rnn_size 512 -word_vec_size 512 \\\\\n","                            -rnn_type LSTM --encoder_type brnn \\\\\n","                            -batch_size 64 -gpu_rank 0 -seed {seed} \\\\\n","                            -learning_rate 0.1 -optim sgd -global_attention general \\\\\n","                            -tensorboard -tensorboard_log_dir \"{log_path}\"\n","\"\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F41mHGQYv9T3","colab_type":"text"},"source":["### GRUS2S\n","\n","As the second model to train, we look at a comparable GRU network, with similar parameters for fair comparison."]},{"cell_type":"code","metadata":{"id":"wl_fF7Uev8gd","colab_type":"code","colab":{}},"source":["# Create training scripts\n","try:\n","  os.remove(f\"code/grus2s_train.sh\")\n","except:\n","  pass\n","\n","for seed in seeds:\n","  for source_path, dev_path, dest_path, _ in datasets:\n","    if \"train\" in source_path:\n","      with open(\"code/grus2s_train.sh\", \"a\") as scriptfile:\n","        data_path = dest_path\n","        model_path = dest_path.replace(f\"{data_dir}/OpenNMT\", f\"models/lstms2s/run_{seed}\")\n","        log_path = dest_path.replace(f\"{data_dir}/OpenNMT\", f\"logs/lstms2s/run_{seed}\")\n","        scriptfile.write(f\"\"\"\n","python OpenNMT-py/train.py  -data \"{data_path}\" -save_model \"{model_path}\" \\\\\n","                            -train_steps 25000 -layers 2 -rnn_size 512 -word_vec_size 512 \\\\\n","                            -rnn_type GRU --encoder_type brnn \\\\\n","                            -batch_size 64 -gpu_rank 0 -seed {seed} \\\\\n","                            -learning_rate 0.1 -optim sgd -global_attention general \\\\\n","                            -tensorboard -tensorboard_log_dir \"{log_path}\"\n","\"\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTkioZCaifW9","colab_type":"text"},"source":["### Large model preprocessing (optional)\n","\n","As large models require more data, these need more steps before training. \n","\n","1.   Please run the `pcfg_big` notebook to generate a dataset five times larger than the original PCFG. This should suffice for the Transformer and possibly ConvS2S as well\n","2.   Run the processing cells below after dataset generation\n","\n","**WARNING!** As these models haven't been trained succesfully, this notebook is set up to the best of our knowledge, but hasn't been fully tested as we did not have access to sufficient computation resources (Google Colab has a maximum of 12 consecutive GPU hours). Be prepared that some script might not work 'out of the box'.[link text](https://)"]},{"cell_type":"code","metadata":{"id":"xFNl_B1QCn7V","colab_type":"code","colab":{}},"source":["# Transformer preprocessing\n","from distutils.dir_util import copy_tree\n","\n","\n","original_dir = f\"{data_dir}/pcfg_big\"\n","transformed_dir = f\"{data_dir}/TransformedBig\"\n","\n","copy_data = False\n","\n","if copy_data:\n","  copy_tree(original_dir, transformed_dir)\n","  for root, dirs, files in os.walk(transformed_dir, topdown=False):\n","    for name in [f for f in files if \".txt\" in f and \"README.txt\" not in f]:\n","        source_file = os.path.join(root, name)\n","        source_file_renamed = source_file\n","        if \"_src_\" in source_file:\n","            source_file_renamed = source_file_renamed.replace(\"_src_\",\"_\").replace(\".txt\", \".src\")\n","        elif \"_tgt_\" in source_file:\n","            source_file_renamed = source_file_renamed.replace(\"_tgt_\",\"_\").replace(\".txt\", \".tgt\")\n","        os.rename(source_file, source_file_renamed)\n","        print(f\"Source '{source_file}' renamed to '{source_file_renamed}'\")\n","\n","\n","# Create list of datasets to preprocess\n","datasets = []\n","dev_set = f\"{transformed_dir}/All/dev5k_all\"\n","\n","for root, dirs, files in os.walk(transformed_dir, topdown=False):\n","  for name in [f for f in files if \".txt\" not in f and \"dev5k\" not in f]:\n","      source_file = os.path.join(root, name)\n","      source_path = source_file.split(\".\")[0]\n","      opennmt_dest_path = source_path.replace(\"/TransformedBig/\",\"/OpenNMTBig/\")\n","      fairseq_dest_path = source_path.replace(\"/TransformedBig/\",\"/FairseqBig/\")\n","      datasets.append((source_path, dev_set, opennmt_dest_path, fairseq_dest_path))\n","      \n","datasets = list(set(datasets))\n","\n","# Remove existing bash files and write new ones\n","try:\n","  os.remove(\"code/opennmt_preprocess_big.sh\")\n","except:\n","  pass\n","\n","for seed in seeds:\n","  for source_path, dev_path, opennmt_dest_path, fairseq_dest_path in datasets:\n","    if \"train\" in source_path:\n","      try:\n","        os.makedirs(opennmt_dest_path)\n","      except:\n","        pass\n","      try:\n","        os.makedirs(fairseq_dest_path)\n","      except:\n","        pass\n","      with open(\"code/fairseq_preprocess_big.sh\", \"a\") as scriptfile:\n","        scriptfile.write(f'python fairseq/preprocess.py  --source-lang src --target-lang tgt --trainpref \"{source_path}\" --validpref \"{dev_path}\" --destdir \"{fairseq_dest_path}\" --seed {seed}\\n')\n","      with open(\"code/opennmt_preprocess_big.sh\", \"a\") as scriptfile:\n","        scriptfile.write(f'python OpenNMT-py/preprocess.py  -train_src \"{source_path}.src\" -train_tgt \"{source_path}.tgt\" -valid_src \"{dev_path}.src\" -valid_tgt \"{dev_path}.tgt\" -save_data \"{opennmt_dest_path}\" -src_vocab_size 70 -tgt_vocab_size 70 --seed {seed}\\n')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYYXnig-hldy","colab_type":"code","colab":{}},"source":["!chmod +x ./code/fairseq_preprocess_big.sh\n","!./code/fairseq_preprocess_big.sh"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEGS4C6NhlcB","colab_type":"code","colab":{}},"source":["!chmod +x ./code/fairseq_preprocess_big.sh\n","!./code/fairseq_preprocess_big.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQEP8tKd5A2V","colab_type":"text"},"source":["### Transformer (optional)\n","\n","As a third comparison, we could train a transformer. This, however, takes far more data and computation time than the models above. If this is desired, the `pcfg_big` notebook can create the necessary data. Below the scripts for preprocesssing can be run seperately."]},{"cell_type":"code","metadata":{"id":"1qhXkd_keOOU","colab_type":"code","colab":{}},"source":["# Train a Transformer for 50000 steps, with 16000 warmup steps\n","# As this training script takes significantly longer, we have\n","# not been able to test its full potential\n","\n","try:\n","  os.remove(\"code/transformer_train.sh\")\n","except:\n","  pass\n","  \n","for seed in seeds:\n","  for source_path, dev_path, dest_path, _ in datasets:\n","    with open(\"code/transformer_train.sh\", \"a\") as scriptfile:\n","      if \"train\" in source_path:\n","        data_path = dest_path\n","        model_path = dest_path.replace(\"data/OpenNMTBig\",f\"models/transformer/run_{seed}\")\n","        log_path = dest_path.replace(\"data/OpenNMTBig\",f\"logs/transformer/run_{seed}\")\n","        scriptfile.write(f\"\"\"\n","python OpenNMT-py/train.py  -data \"{data_path}\" -save_model \"{model_path}\" \\\\\n","                            -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 \\\\\n","                            -encoder_type transformer -decoder_type transformer -position_encoding \\\\\n","                            -train_steps 50000 -max_generator_batches 2 -dropout 0.1 -batch_size 4096 \\\\\n","                            -batch_type tokens -normalization tokens -accum_count 2 -optim adam \\\\\n","                            -adam_beta2 0.998 -decay_method noam -warmup_steps 16000 -learning_rate 2 \\\\\n","                            -max_grad_norm 0 -param_init 0 -param_init_glorot -label_smoothing 0.1 \\\\\n","                            -valid_steps 1000 -save_checkpoint_steps 5000 -world_size 1 -gpu_rank 0 -seed {seed} \\\\\n","                            -tensorboard -tensorboard_log_dir \"{log_path}\"\n","  \"\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rQX-EaPmxCJH"},"source":["### ConvS2S (optional)\n","\n","This model has been trained on the original PCFG, but we were unable to validate its performance and were informed that accuracies might not be sufficient as convolutional networks require more training data. We have set up this notebook to use the new 'Big' data set, but have not tested its performance or functionality."]},{"cell_type":"code","metadata":{"id":"HarntSMt89KS","colab_type":"code","colab":{}},"source":["# Train a convolutional model based on Fairseq\n","# We have been able to train this model on a small\n","# dataset, but have not been able to validate its\n","# performance\n","\n","try:\n","  os.remove(\"code/convs2s_train.sh\")\n","except:\n","  pass\n","  \n","for seed in seeds:\n","  for source_path, dev_path, dest_path in datasets:\n","    with open(\"code/convs2s_train.sh\", \"a\") as scriptfile:\n","      if \"train\" in source_path:\n","        data_path = dest_path\n","        model_path = dest_path.replace(\"data/FairseqBig\",f\"models/convs2s/run_{seed}\")\n","        log_path = dest_path.replace(\"data/FairseqBig\",f\"logs/convs2s/run_{seed}\")\n","        scriptfile.write(f\"\"\"\n","python fairseq/train.py \"{data_path}\" --no-epoch-checkpoints --arch fconv_wmt_en_de --lr 0.25 --max-tokens 3000 --save-dir \"{model_path}\" \\\\\n","                                      --clip-norm 0.1 --dropout 0.1 --max-epoch 25 --save-interval 1 --no-epoch-checkpoints \\\\\n","                                      --encoder-embed-dim 512 --decoder-embed-dim 512 \\\\\n","                                      --tensorboard-logdir \"{log_path}\" \\\\\n","                                      --batch-size 64 --seed {seed}\n","\"\"\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FP9jBAMpQmM-","colab_type":"text"},"source":["## Model Training\n","\n","Training all models, with optional training for the Transformer and ConvS2S."]},{"cell_type":"markdown","metadata":{"id":"ddlvTK8MXhIj","colab_type":"text"},"source":["### LSTMS2S"]},{"cell_type":"code","metadata":{"id":"EOr8Dh3bQlop","colab_type":"code","colab":{}},"source":["!chmod +x ./code/lstms2s_train.sh\n","!./code/lstms2s_train.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vB8_zFnvksHi","colab_type":"text"},"source":["### GRUS2S"]},{"cell_type":"code","metadata":{"id":"UqZIn7UakuJK","colab_type":"code","colab":{}},"source":["!chmod +x ./code/grus2s_train.sh\n","!./code/grus2s_train.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxoarW_-dGTp","colab_type":"text"},"source":["### Transformer (optional)"]},{"cell_type":"code","metadata":{"id":"CzOtsnSrdIBP","colab_type":"code","colab":{}},"source":["!chmod +x ./code/transformer_train.sh\n","!./code/transformer_train.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuS27GM6XmF2","colab_type":"text"},"source":["### ConvS2S (optional)"]},{"cell_type":"code","metadata":{"id":"1kGKBWNKW72Z","colab_type":"code","colab":{}},"source":["!chmod +x ./code/convs2s_train.sh\n","!./code/convs2s_train.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olcce8_nX1Yl","colab_type":"text"},"source":["## Evaluation\n","\n","These scripts were used for initial evaluation and experimentation. They do not contribute to training, but are left for convenience"]},{"cell_type":"code","metadata":{"id":"l3exPKH91iys","colab_type":"code","colab":{}},"source":["def sequence_accuracy(a, b):\n","    return float(a.split() == b.split())\n","\n","def word_accuracy(a, b):\n","    return float(sum([1. for _ in a.split() if _ in b.split()]) / len(b.split()))\n","\n","import numpy as np\n","\n","def levenshtein(seq1, seq2):\n","    size_x = len(seq1) + 1\n","    size_y = len(seq2) + 1\n","    matrix = np.zeros ((size_x, size_y))\n","    for x in range(size_x):\n","        matrix [x, 0] = x\n","    for y in range(size_y):\n","        matrix [0, y] = y\n","\n","    for x in range(1, size_x):\n","        for y in range(1, size_y):\n","            if seq1[x-1] == seq2[y-1]:\n","                matrix [x,y] = min(\n","                    matrix[x-1, y] + 1,\n","                    matrix[x-1, y-1],\n","                    matrix[x, y-1] + 1\n","                )\n","            else:\n","                matrix [x,y] = min(\n","                    matrix[x-1,y] + 1,\n","                    matrix[x-1,y-1] + 1,\n","                    matrix[x,y-1] + 1\n","                )\n","    # print (matrix)\n","    return (matrix[size_x - 1, size_y - 1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"za3feOe9frXU","colab_type":"code","outputId":"0f17b470-80c0-4d1f-d5db-9f2d5421a369","executionInfo":{"status":"ok","timestamp":1589441178127,"user_tz":-120,"elapsed":45680,"user":{"displayName":"Maarten Peters","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhV6rPMqReqK9fDeJMdSZbZknKdyi19u_-I3fymng=s64","userId":"03382078059805531082"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Analyzing predictions\n","from os import listdir\n","from os.path import isfile, join\n","\n","\n","test_dir = f'{data_dir}/Final/All/'\n","for seed in seeds:\n","  for model in models:\n","    pred_dir = f'results/run_{seed}/All/'\n","    with open(join(test_dir, \"test5k_all.tgt\")) as input_file:\n","      pred = input_file.readlines()\n","    \n","    with open(join(pred_dir, \"test5k_all.prd\")) as input_file:\n","      actual = input_file.readlines()\n","\n","    comparison = zip(pred, actual)\n","\n","    sequence_acc = 0\n","    word_acc = 0\n","    lev_scores = []\n","    for i, c in enumerate(comparison):\n","      p, a = c\n","      sequence_acc += sequence_accuracy(p, a)\n","      word_acc += word_accuracy(p, a)\n","      lev_scores.append(levenshtein(p, a))\n","\n","    sequence_acc /= len(pred)\n","    word_acc /= len(pred)\n","    lev_score = sum(lev_scores) / len(lev_scores)\n","\n","    print(f'{model}, seed {seed}: Seq. acc.: {sequence_acc:.4f}, Word acc.: {word_acc:.4f}, Avg. Lev.: {lev_score:.4f}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LSTMS2S: Seq. acc.: 0.6382, Word acc.: 0.9477, Avg. Lev.: 3.8848\n"],"name":"stdout"}]}]}